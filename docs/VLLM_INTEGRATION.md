# vLLM é›†æˆæŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨ vLLM ä½œä¸º LLM åŽç«¯è¿è¡Œ Work Agentï¼Œæ›¿ä»£ OpenAI å®˜æ–¹ APIã€‚

## ä¸ºä»€ä¹ˆä½¿ç”¨ vLLMï¼Ÿ

- âœ… **æœ¬åœ°éƒ¨ç½²**: æ•°æ®ä¸å‡ºæœ¬åœ°ï¼Œä¿æŠ¤éšç§
- âœ… **æˆæœ¬èŠ‚çœ**: æ— éœ€æ”¯ä»˜ OpenAI API è´¹ç”¨
- âœ… **é«˜æ€§èƒ½**: GPU åŠ é€ŸæŽ¨ç†ï¼Œæ”¯æŒ PagedAttention
- âœ… **å…¼å®¹æ€§**: æä¾› OpenAI å…¼å®¹ API
- âœ… **å¼€æºæ¨¡åž‹**: æ”¯æŒ Qwenã€Llamaã€Mistral ç­‰å¼€æºæ¨¡åž‹

## å‰ç½®è¦æ±‚

### 1. ç¡¬ä»¶è¦æ±‚

- **GPU**: NVIDIA GPU (æŽ¨è 16GB+ æ˜¾å­˜ç”¨äºŽ 7B æ¨¡åž‹)
- **CPU**: å¯é€‰ï¼Œæ€§èƒ½è¾ƒä½Ž
- **å†…å­˜**: æŽ¨è 32GB+ RAM

### 2. è½¯ä»¶ä¾èµ–

```bash
# å®‰è£… vLLM
pip install vllm

# æˆ–ä½¿ç”¨ conda
conda install vllm -c conda-forge
```

## å¿«é€Ÿå¼€å§‹

### æ­¥éª¤ 1: å¯åŠ¨ vLLM æœåŠ¡

#### æ–¹æ³• 1: Python å‘½ä»¤

```bash
# ä½¿ç”¨ Qwen2.5-7B-Instruct (æŽ¨èï¼Œæ”¯æŒ function calling)
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-7B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --served-model-name Qwen/Qwen2.5-7B-Instruct \
    --enable-auto-tool-choice \
    --tool-call-parser hermes

# å…¶ä»–æ”¯æŒ function calling çš„æ¨¡åž‹
# - meta-llama/Llama-3.1-8B-Instruct
# - mistralai/Mistral-7B-Instruct-v0.3
# - NousResearch/Hermes-2-Pro-Llama-3-8B
```

**é‡è¦å‚æ•°è¯´æ˜Ž:**
- `--enable-auto-tool-choice`: å¯ç”¨å·¥å…·è°ƒç”¨
- `--tool-call-parser hermes`: ä½¿ç”¨ Hermes æ ¼å¼è§£æžå·¥å…·è°ƒç”¨
- `--served-model-name`: æ¨¡åž‹åç§°ï¼ˆéœ€ä¸Ž Agent é…ç½®ä¸€è‡´ï¼‰

#### æ–¹æ³• 2: Docker

```bash
# æ‹‰å–é•œåƒ
docker pull vllm/vllm-openai:latest

# å¯åŠ¨æœåŠ¡
docker run --gpus all -p 8000:8000 \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    vllm/vllm-openai:latest \
    --model Qwen/Qwen2.5-7B-Instruct \
    --enable-auto-tool-choice \
    --tool-call-parser hermes
```

#### éªŒè¯æœåŠ¡

```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:8000/v1/models

# æµ‹è¯• chat completions
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen2.5-7B-Instruct",
        "messages": [{"role": "user", "content": "Hello"}]
    }'
```

### æ­¥éª¤ 2: é…ç½®çŽ¯å¢ƒå˜é‡

ç¼–è¾‘ `.env` æ–‡ä»¶:

```bash
# vLLM é…ç½®
OPENAI_API_KEY=EMPTY  # vLLM ä¸éœ€è¦çœŸå®ž key
OPENAI_API_BASE=http://localhost:8000/v1
AGENT_MODEL=Qwen/Qwen2.5-7B-Instruct

# Weather API (å¦‚æžœè¦æµ‹è¯• weather tool)
WEATHER_API_KEY=your_weather_api_key
```

### æ­¥éª¤ 3: è¿è¡Œ Agent

#### ä½¿ç”¨æµ‹è¯•è„šæœ¬ï¼ˆæŽ¨èï¼‰

```bash
# è®¾ç½®çŽ¯å¢ƒå˜é‡
export WEATHER_API_KEY=your_api_key

# è¿è¡Œæµ‹è¯•
.venv/bin/python scripts/test_vllm_agent.py

# æˆ–æŒ‡å®šå‚æ•°
.venv/bin/python scripts/test_vllm_agent.py \
    --base-url http://localhost:8000/v1 \
    --model Qwen/Qwen2.5-7B-Instruct \
    --query "æŸ¥è¯¢ä¸Šæµ·çš„å¤©æ°”"
```

#### ä½¿ç”¨ CLI

```bash
# æ¿€æ´»è™šæ‹ŸçŽ¯å¢ƒ
source .venv/bin/activate

# åˆ—å‡ºå·¥å…·
python -m work_agent list-tools

# å•æ¬¡æŸ¥è¯¢
python -m work_agent run "æŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”"

# äº¤äº’æ¨¡å¼
python -m work_agent repl
```

## æŽ¨èæ¨¡åž‹

### æ”¯æŒ Function Calling çš„æ¨¡åž‹

| æ¨¡åž‹ | å‚æ•°é‡ | æ˜¾å­˜éœ€æ±‚ | æŽ¨èåœºæ™¯ |
|------|--------|----------|----------|
| Qwen/Qwen2.5-7B-Instruct | 7B | ~16GB | **æŽ¨èï¼Œä¸­æ–‡æ•ˆæžœå¥½** |
| Qwen/Qwen2.5-14B-Instruct | 14B | ~28GB | æ›´å¼ºæ€§èƒ½ |
| meta-llama/Llama-3.1-8B-Instruct | 8B | ~16GB | è‹±æ–‡åœºæ™¯ |
| mistralai/Mistral-7B-Instruct-v0.3 | 7B | ~16GB | å¹³è¡¡é€‰æ‹© |
| NousResearch/Hermes-2-Pro-Llama-3-8B | 8B | ~16GB | å·¥å…·è°ƒç”¨ä¼˜åŒ– |

### æ¨¡åž‹é€‰æ‹©å»ºè®®

1. **ä¸­æ–‡åœºæ™¯**: ä¼˜å…ˆé€‰æ‹© Qwen ç³»åˆ—
2. **è‹±æ–‡åœºæ™¯**: Llama 3.1 æˆ– Mistral
3. **æ˜¾å­˜æœ‰é™**: 7B-8B æ¨¡åž‹
4. **è¿½æ±‚æ€§èƒ½**: 14B+ æ¨¡åž‹æˆ–é‡åŒ–ç‰ˆæœ¬

## é«˜çº§é…ç½®

### 1. æ€§èƒ½ä¼˜åŒ–

```bash
# å¯ç”¨ GPU åŠ é€Ÿ
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-7B-Instruct \
    --tensor-parallel-size 2 \  # å¤š GPU å¹¶è¡Œ
    --gpu-memory-utilization 0.9 \  # GPU æ˜¾å­˜åˆ©ç”¨çŽ‡
    --max-model-len 4096 \  # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
    --enable-auto-tool-choice \
    --tool-call-parser hermes
```

### 2. é‡åŒ–æ¨¡åž‹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰

```bash
# ä½¿ç”¨ 4-bit é‡åŒ–
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-7B-Instruct-AWQ \
    --quantization awq \
    --enable-auto-tool-choice \
    --tool-call-parser hermes
```

### 3. æ‰¹å¤„ç†ä¼˜åŒ–

```bash
# å¢žåŠ æ‰¹å¤„ç†å¤§å°
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-7B-Instruct \
    --max-num-batched-tokens 8192 \
    --max-num-seqs 256 \
    --enable-auto-tool-choice \
    --tool-call-parser hermes
```

### 4. è¿œç¨‹éƒ¨ç½²

å¦‚æžœ vLLM æœåŠ¡éƒ¨ç½²åœ¨è¿œç¨‹æœåŠ¡å™¨:

```bash
# .env é…ç½®
OPENAI_API_BASE=http://your-server-ip:8000/v1
```

## æ•…éšœæŽ’æŸ¥

### é—®é¢˜ 1: æ¨¡åž‹ä¸æ”¯æŒ function calling

**é”™è¯¯ä¿¡æ¯**: `Tool calls are not supported for this model`

**è§£å†³æ–¹æ¡ˆ**:
1. ä½¿ç”¨æ”¯æŒ function calling çš„æ¨¡åž‹ï¼ˆè§æŽ¨èåˆ—è¡¨ï¼‰
2. ç¡®ä¿å¯åŠ¨æ—¶æ·»åŠ  `--enable-auto-tool-choice --tool-call-parser hermes`

### é—®é¢˜ 2: æ˜¾å­˜ä¸è¶³

**é”™è¯¯ä¿¡æ¯**: `CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. ä½¿ç”¨é‡åŒ–æ¨¡åž‹
--quantization awq

# 2. å‡å°‘æ˜¾å­˜åˆ©ç”¨çŽ‡
--gpu-memory-utilization 0.8

# 3. å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦
--max-model-len 2048

# 4. ä½¿ç”¨æ›´å°çš„æ¨¡åž‹
```

### é—®é¢˜ 3: è¿žæŽ¥è¶…æ—¶

**é”™è¯¯ä¿¡æ¯**: `Connection timeout`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯åŠ¨
curl http://localhost:8000/v1/models

# 2. æ£€æŸ¥é˜²ç«å¢™
sudo ufw allow 8000

# 3. å¢žåŠ è¶…æ—¶æ—¶é—´
# åœ¨ config.py ä¸­æ·»åŠ é…ç½®
```

### é—®é¢˜ 4: å·¥å…·è°ƒç”¨æ ¼å¼é”™è¯¯

**ç—‡çŠ¶**: Agent æ— æ³•æ­£ç¡®è°ƒç”¨å·¥å…·

**è§£å†³æ–¹æ¡ˆ**:
1. å°è¯•ä¸åŒçš„ `--tool-call-parser`:
   - `hermes`
   - `mistral`
   - `llama3_json`
2. æ£€æŸ¥æ¨¡åž‹æ˜¯å¦çœŸæ­£æ”¯æŒ function calling
3. æŸ¥çœ‹ vLLM æ—¥å¿—äº†è§£è¯¦ç»†é”™è¯¯

## æ€§èƒ½å¯¹æ¯”

| åŽç«¯ | å»¶è¿Ÿ (é¦– token) | åžåé‡ | æˆæœ¬ | éšç§ |
|------|----------------|--------|------|------|
| OpenAI API | ~200ms | é«˜ | ðŸ’°ðŸ’°ðŸ’° | âš ï¸ |
| vLLM (æœ¬åœ°) | ~50ms | å¾ˆé«˜ | ðŸ’° (ç¡¬ä»¶) | âœ… |
| vLLM (è¿œç¨‹) | ~100ms | å¾ˆé«˜ | ðŸ’°ðŸ’° | âœ… |

## ç”Ÿäº§éƒ¨ç½²å»ºè®®

1. **ä½¿ç”¨è´Ÿè½½å‡è¡¡**: Nginx/HAProxy åˆ†å‘è¯·æ±‚åˆ°å¤šä¸ª vLLM å®žä¾‹
2. **ç›‘æŽ§æŒ‡æ ‡**: Prometheus + Grafana ç›‘æŽ§ GPU ä½¿ç”¨çŽ‡ã€å»¶è¿Ÿç­‰
3. **æ—¥å¿—æ”¶é›†**: ELK/Loki æ”¶é›† vLLM å’Œ Agent æ—¥å¿—
4. **å®¹å™¨åŒ–**: Docker/K8s éƒ¨ç½²ï¼Œä¾¿äºŽæ‰©å±•
5. **å¤‡ä»½æ–¹æ¡ˆ**: é…ç½® OpenAI API ä½œä¸º fallback

## ç¤ºä¾‹ï¼šå®Œæ•´éƒ¨ç½²è„šæœ¬

```bash
#!/bin/bash
# deploy_vllm.sh - ä¸€é”®éƒ¨ç½² vLLM Agent

set -e

echo "=== vLLM Agent éƒ¨ç½²è„šæœ¬ ==="

# 1. æ£€æŸ¥ GPU
if ! command -v nvidia-smi &> /dev/null; then
    echo "é”™è¯¯: æœªæ£€æµ‹åˆ° NVIDIA GPU"
    exit 1
fi

# 2. å®‰è£… vLLM
pip install vllm -q

# 3. ä¸‹è½½æ¨¡åž‹ï¼ˆè‡ªåŠ¨ç¼“å­˜åˆ° ~/.cache/huggingfaceï¼‰
echo "ä¸‹è½½æ¨¡åž‹..."
python -c "from transformers import AutoTokenizer; AutoTokenizer.from_pretrained('Qwen/Qwen2.5-7B-Instruct')"

# 4. å¯åŠ¨ vLLM æœåŠ¡
echo "å¯åŠ¨ vLLM æœåŠ¡..."
nohup python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-7B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --enable-auto-tool-choice \
    --tool-call-parser hermes \
    > vllm.log 2>&1 &

VLLM_PID=$!
echo "vLLM æœåŠ¡å·²å¯åŠ¨ (PID: $VLLM_PID)"

# 5. ç­‰å¾…æœåŠ¡å°±ç»ª
echo "ç­‰å¾…æœåŠ¡å¯åŠ¨..."
for i in {1..30}; do
    if curl -s http://localhost:8000/v1/models > /dev/null; then
        echo "âœ… vLLM æœåŠ¡å°±ç»ª"
        break
    fi
    sleep 2
done

# 6. é…ç½® Agent
cat > .env << EOF
OPENAI_API_KEY=EMPTY
OPENAI_API_BASE=http://localhost:8000/v1
AGENT_MODEL=Qwen/Qwen2.5-7B-Instruct
WEATHER_API_KEY=${WEATHER_API_KEY}
EOF

echo "âœ… éƒ¨ç½²å®Œæˆï¼"
echo ""
echo "ä½¿ç”¨æ–¹æ³•:"
echo "  python -m work_agent run 'æŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”'"
echo ""
echo "åœæ­¢æœåŠ¡:"
echo "  kill $VLLM_PID"
```

## å‚è€ƒèµ„æº

- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [OpenAI å…¼å®¹ API](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html)
- [Function Calling æ”¯æŒ](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#tool-calling)
- [Qwen æ¨¡åž‹](https://huggingface.co/Qwen)

## å¸¸è§é—®é¢˜

**Q: vLLM å’Œ OpenAI API æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ**

A: vLLM æ˜¯æœ¬åœ°æŽ¨ç†æœåŠ¡å™¨ï¼Œæä¾›ä¸Ž OpenAI å…¼å®¹çš„ APIã€‚ä¼˜åŠ¿æ˜¯æ•°æ®æœ¬åœ°åŒ–ã€æˆæœ¬å¯æŽ§ï¼ŒåŠ£åŠ¿æ˜¯éœ€è¦è‡ªå·±ç»´æŠ¤ç¡¬ä»¶å’ŒæœåŠ¡ã€‚

**Q: å¦‚ä½•é€‰æ‹©æ¨¡åž‹ï¼Ÿ**

A: æ ¹æ®åœºæ™¯é€‰æ‹©ï¼š
- ä¸­æ–‡åœºæ™¯ â†’ Qwen
- è‹±æ–‡åœºæ™¯ â†’ Llama/Mistral
- æ˜¾å­˜æœ‰é™ â†’ 7B æ¨¡åž‹ + é‡åŒ–
- è¿½æ±‚æ€§èƒ½ â†’ 14B+ æ¨¡åž‹

**Q: å¯ä»¥åŒæ—¶ä½¿ç”¨ OpenAI å’Œ vLLM å—ï¼Ÿ**

A: å¯ä»¥ã€‚é€šè¿‡ä¿®æ”¹ `OPENAI_API_BASE` çŽ¯å¢ƒå˜é‡åˆ‡æ¢åŽç«¯ï¼Œæˆ–å®žçŽ°è·¯ç”±é€»è¾‘åŠ¨æ€é€‰æ‹©ã€‚

**Q: function calling æ•ˆæžœä¸å¥½æ€Žä¹ˆåŠžï¼Ÿ**

A:
1. ç¡®è®¤æ¨¡åž‹çœŸæ­£æ”¯æŒ function calling
2. å°è¯•ä¸åŒçš„ `--tool-call-parser`
3. è°ƒæ•´ prompt engineering
4. ä½¿ç”¨ä¸“é—¨ä¼˜åŒ–è¿‡çš„æ¨¡åž‹ï¼ˆå¦‚ Hermes ç³»åˆ—ï¼‰
